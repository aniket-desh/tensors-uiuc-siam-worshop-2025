{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30ea9a43",
   "metadata": {},
   "source": [
    "## Supervised Learning with ITensor: MPS for Classification\n",
    "\n",
    "[Supervised Learning with Quantum-Inspired Tensor Networks](https://arxiv.org/pdf/1605.05775), *NeurIPS 2016.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13d003b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random, Statistics\n",
    "using ITensors, ITensorMPS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19596849",
   "metadata": {},
   "source": [
    "*Why machine learning?* Tensor trains provide a linear model from a very large feature space, with only $O(N dm^2)$ parameters. \n",
    "\n",
    "*Goal:* Classify $8\\times 8$ grayscale images into two classes, can be extended to multi-class classification (e.g. MNIST)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68c104b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# synthetic data generation\n",
    "\n",
    "const H, W = 8, 8\n",
    "function make_bar(imgtype::Symbol; rng=Random.default_rng())\n",
    "    X = fill(0.0, H, W)\n",
    "    if imgtype == :vertical\n",
    "        j = rand(rng, 2:W-1)\n",
    "        X[:, j] .= 1.0\n",
    "    elseif imgtype == :horizontal\n",
    "        i = rand(rng, 2:H-1)\n",
    "        X[i, :] .= 1.0\n",
    "    end\n",
    "    return X\n",
    "end\n",
    "\n",
    "function synth_dataset(n_per=200; rng=Random.default_rng())\n",
    "    Xv = [make_bar(:vertical; rng=rng) for _ in 1:n_per]\n",
    "    Xh = [make_bar(:horizontal; rng=rng) for _ in 1:n_per]\n",
    "    X = vcat(Xv, Xh)\n",
    "    y = vcat(fill(1, n_per), fill(0, n_per)) # 1 = vertical, 0 = horizontal\n",
    "    shuffle = randperm(rng, length(X))\n",
    "    return X[shuffle], y[shuffle]\n",
    "end\n",
    "\n",
    "X, y = synth_dataset(200)\n",
    "N = H * W # number of pixels, i.e. number of MPS sites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb11283",
   "metadata": {},
   "source": [
    "*How are tensors useful?* Let $x = (x_1, x_2, \\ldots, x_N) \\in \\mathbb R^N$ be an input data point (image with $N$ pixels). A length $N$ binary vector has $2^N$ possible configurations, which is infeasible to work with directly. \n",
    "\n",
    "The trick is the *lift* each scalar $x_j$ to a small local feature vector and use a *tensor product* to build a structured, high-dimensional feature $\\Phi(x)$ that can be efficiently represented as a matrix product state (MPS).\n",
    "\n",
    "*Local feature map:*\n",
    "$$ \\phi: \\mathbb R \\to \\mathbb R^d,\\quad x_j \\mapsto \\phi(x_j) = \\left(\\phi_1(x_j), \\ldots, \\phi_d(x_j)\\right) $$\n",
    "We develop a tensor-product (rank-1, order-$N$ tensor) feature map:\n",
    "$$ \\Phi(x)_{s_1, s_2, \\ldots, s_N} = \\left[ \\phi(x_1) \\otimes \\cdots \\otimes \\phi(x_N) \\right]_{s_1, s_2, \\ldots, s_N} = \\prod_{j=1}^N \\phi_{s_j}(x_j),\\quad s_j \\in \\{1, 2, \\ldots, d\\} $$\n",
    "\n",
    "For grayscale images, a simple $d=2$ choice is\n",
    "$$ \\phi(x_j) = \\left( \\cos\\left(\\frac{\\pi}{2} x_j\\right), \\sin\\left(\\frac{\\pi}{2} x_j\\right) \\right) \\in \\mathbb R^2 $$\n",
    "which is *normalized*, i.e. $\\|\\phi(x_j)\\|_2 = 1$.\n",
    "\n",
    "*Why is this helpful?* $\\Phi(x)$ lives in a $d^N$-dimensional space, which is huge even for small $N$. However, the tensor-product structure allows us to represent $\\Phi(x)$ efficiently as an MPS with low bond dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32b8a440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@inline feature(x::Real) = (cospi(0.5 * x), sinpi(0.5 * x)) # d = 2 feature map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32bb7d3",
   "metadata": {},
   "source": [
    "We classify data points with a *linear model* in the lifted space.\n",
    "$$ f(x) = W \\cdot \\Phi(x) = \\sum_{s_1, s_2, \\ldots, s_N} W_{s_1, s_2, \\ldots, s_N} \\Phi(x)_{s_1, s_2, \\ldots, s_N} $$\n",
    "where $W$ is a weight tensor of the same order and dimension as $\\Phi(x)$. To avoid working with the full $W$, we represent it as an MPS with low bond dimension $m$:\n",
    "$$ W_{s_1, s_2, \\ldots, s_N} = \\sum_{\\{\\alpha\\}} A^{[1]}_{s_1, \\alpha_1} A^{[2]}_{\\alpha_1, s_2, \\alpha_2} \\cdots A^{[N]}_{\\alpha_{N-1}, s_N} $$\n",
    "This reduces the number of parameters from $d^N$ to $O(N d m^2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4660234a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "acc (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# local 2-dim indices for each pixel\n",
    "sites = siteinds(\"Qubit\", N) # qubit sites have dim 2\n",
    "\n",
    "# random weight MPS with bond dimension m\n",
    "m = 8\n",
    "Wmps = randomMPS(sites, m) # normalized random MPS (our weight tensor)\n",
    "\n",
    "# turn an image into 1-site feature Itensors matching 'sites'\n",
    "function image_to_features(img::AbstractMatrix{<:Real}, sites::Vector{<:Index})\n",
    "    feats = ITensor[]\n",
    "    v = vec(img)\n",
    "    for n in 1:length(v)\n",
    "        s = sites[n]\n",
    "        a, b = feature(v[n])\n",
    "        phi = ITensor(s)\n",
    "        phi[s => 1] = a\n",
    "        phi[s => 2] = b\n",
    "        push!(feats, phi)\n",
    "    end\n",
    "    return feats\n",
    "end\n",
    "\n",
    "# contract features with Wmps to get a scalar score f(x)\n",
    "function score(W::MPS, img::AbstractMatrix)\n",
    "    feats = image_to_features(img, siteinds(W))\n",
    "    T = nothing\n",
    "    @inbounds for n in 1:length(feats)\n",
    "        T = isnothing(T) ? W[n] * feats[n] : T * W[n] * feats[n]\n",
    "    end\n",
    "    return sum(Array(T)) # scalar output\n",
    "end\n",
    "\n",
    "pred(s) = s > 0 ? 1 : 0 # binary prediction from score\n",
    "acc(W, Xs, ys) = mean(pred(score(W, x)) == y for (x, y) in zip(Xs, ys)) # accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e57a790",
   "metadata": {},
   "source": [
    "To factor a big tensor $\\Psi_{s_1, \\ldots, s_N}$ to an MPS, we use a sequence of SVDs (tensor train SVD).\n",
    "\n",
    "The standard algorithm for this is as follows\n",
    "1. group $(s_1)$ and $(s_2, \\ldots, s_N)$\n",
    "2. SVD to get $U\\Sigma V^T$ and set $A^{[1]} = U\\Sigma$\n",
    "3. absorb $V^T$ into the next tensor and repeat until all tensors are extracted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3b80d5",
   "metadata": {},
   "source": [
    "For binary labels $y_n \\in \\{0, 1\\}$, we use a *squared loss*\n",
    "$C(W) = \\frac{1}{2} \\sum_{n=1}^{N_T} (f(x_n) - y_n)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c172961a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: initial acc\n",
      "│   acc(Wmps, X, y) = 0.235\n",
      "└ @ Main /Users/aniket/Documents/university/siam/julia-tensors-worshop/notebooks/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X14sZmlsZQ==.jl:66\n"
     ]
    },
    {
     "ename": "MethodError",
     "evalue": "MethodError: no method matching *(::Nothing, ::ITensor)\n\nClosest candidates are:\n  *(::Any, ::Any, !Matched::Any, !Matched::Any...)\n   @ Base operators.jl:587\n  *(!Matched::ITensor, ::ITensor)\n   @ ITensors ~/.julia/packages/ITensors/iPhbw/src/tensor_operations/tensor_algebra.jl:60\n  *(!Matched::ChainRulesCore.NoTangent, ::Any)\n   @ ChainRulesCore ~/.julia/packages/ChainRulesCore/Vsbj9/src/tangent_arithmetic.jl:64\n  ...\n",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching *(::Nothing, ::ITensor)\n",
      "\n",
      "Closest candidates are:\n",
      "  *(::Any, ::Any, !Matched::Any, !Matched::Any...)\n",
      "   @ Base operators.jl:587\n",
      "  *(!Matched::ITensor, ::ITensor)\n",
      "   @ ITensors ~/.julia/packages/ITensors/iPhbw/src/tensor_operations/tensor_algebra.jl:60\n",
      "  *(!Matched::ChainRulesCore.NoTangent, ::Any)\n",
      "   @ ChainRulesCore ~/.julia/packages/ChainRulesCore/Vsbj9/src/tangent_arithmetic.jl:64\n",
      "  ...\n",
      "\n",
      "\n",
      "Stacktrace:\n",
      " [1] *(::Nothing, ::ITensor, ::ITensor)\n",
      "   @ Base ./operators.jl:587\n",
      " [2] update_site!(W::MPS, j::Int64; batch::SubArray{Int64, 1, Vector{Int64}, Tuple{UnitRange{Int64}}, true}, lr::Float64)\n",
      "   @ Main ~/Documents/university/siam/julia-tensors-worshop/notebooks/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X14sZmlsZQ==.jl:33\n",
      " [3] train_one_site!(W::MPS; epochs::Int64, batchsize::Int64, rng::TaskLocalRNG)\n",
      "   @ Main ~/Documents/university/siam/julia-tensors-worshop/notebooks/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X14sZmlsZQ==.jl:58\n",
      " [4] top-level scope\n",
      "   @ ~/Documents/university/siam/julia-tensors-worshop/notebooks/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X14sZmlsZQ==.jl:67"
     ]
    }
   ],
   "source": [
    "# one-site training loop\n",
    "\n",
    "Xphi = [image_to_features(x, sites) for x in X] # precompute features\n",
    "\n",
    "function left_env(W::MPS, feats::Vector{ITensor}, j::Int)\n",
    "    T = ITensor()\n",
    "    for n in 1:j-1\n",
    "        T = n == 1 ? W[n] * feats[n] : T * W[n] * feats[n]\n",
    "    end\n",
    "    return T\n",
    "end\n",
    "\n",
    "function right_env(W::MPS, feats::Vector{ITensor}, j::Int)\n",
    "    T = ITensor()\n",
    "    for n in length(feats):-1:j+1\n",
    "        T = n == length(feats) ? W[n] * feats[n] : W[n] * feats[n] * T\n",
    "    end\n",
    "    return T\n",
    "end\n",
    "\n",
    "# helper to make indices of A and B match for contraction\n",
    "_align_to(to::ITensor, ref::ITensor) = replaceinds(to, uniqueinds(to) .=> uniqueinds(ref))\n",
    "\n",
    "# one-site gradient step at site j using small mini-batch\n",
    "function update_site!(W::MPS, j; batch, lr=5e-3)\n",
    "    G = zero(W[j]) # gradient accumulator\n",
    "    for t in batch\n",
    "        feats = Xphi[t]\n",
    "        \n",
    "        # prediction\n",
    "        T = nothing\n",
    "        @inbounds for n in 1:length(feats)\n",
    "            T = isnothing(T) > W[n] * feats[n] : T * W[n] * feats[n]\n",
    "        end\n",
    "        yhat = sum(Array(T))\n",
    "        e = (y[t] - yhat) # error\n",
    "\n",
    "        L = left_env(W, feats, j)\n",
    "        R = right_env(W, feats, j)\n",
    "        phi = feats[j]\n",
    "\n",
    "        contrib = isemptyinds(L) ? phi : L * phi\n",
    "        contrib = isemptyinds(R) ? contrib : contrib * R\n",
    "        G += -e * _align_to(contrib, W[j])\n",
    "    end\n",
    "    new_Wj = W[j] - lr * G\n",
    "    replacebond!(W, j, new_Wj; maxdim=m, cutoff=1e-12)\n",
    "    return W\n",
    "end\n",
    "\n",
    "# sweep training\n",
    "function train_one_site!(W::MPS; epochs=4, batchsize=128, rng=Random.default_rng())\n",
    "    idxs = collect(1:length(X))\n",
    "    for ep in 1:epochs\n",
    "        shuffle!(rng, idxs)\n",
    "        for j in 1:length(W)\n",
    "            batch = @view idxs[1:min(batchsize, length(idxs))]\n",
    "            update_site!(W, j; batch=batch)\n",
    "            rotate!(idxs, min(batchsize, length(idxs)))\n",
    "        end\n",
    "        @info \"epoch=$ep  acc=$(acc(W, X, y))\"\n",
    "    end\n",
    "    return W\n",
    "end\n",
    "\n",
    "@info \"initial acc\" acc(Wmps, X, y)\n",
    "train_one_site!(Wmps; epochs=3)\n",
    "@info \"final acc\"   acc(Wmps, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "529ec4cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
