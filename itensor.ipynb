{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07e6e78e",
   "metadata": {},
   "source": [
    "## ITensor and Tensor Network Diagrams\n",
    "ITensor is a library for efficient tensor computations, particularly useful in quantum physics and numerical simulations. First, we look at tensor network diagrams.\n",
    "\n",
    "*All diagrams sourced from [tensornetworks.org](https://tensornetwork.org/diagrams/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52cd97b",
   "metadata": {},
   "source": [
    "Let's quickly review some important notation: Einstein summation convention.\n",
    "\n",
    "> *\"I have made a great discovery in mathematics; I have suppressed the summation sign every time that the summation must be made over an index which occurs twice...\"*\n",
    "\n",
    "To simplify expressions involving tensors with many indices, we hide the summation symbol $\\sum$ by *implying* that whenever an index variable appears twice in a single term, it implies summation over all the values of that index. For example, matrix multiplication becomes:\n",
    "$$ C_{ik} = \\sum_j A_{ij} B_{jk} \\implies C_{ik} = A_{ij} B_{jk} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897f9042",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "  <img src=\"figs/types.png\" alt=\"Types\" width=\"45%\">\n",
    "  <img src=\"figs/diagrams.png\" alt=\"Diagrams\" width=\"45%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802927a2",
   "metadata": {},
   "source": [
    "Let's implement the above diagrams using ITensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "596b8f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.24233830315135"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using ITensors\n",
    "\n",
    "# vector v_j\n",
    "j = Index(4, \"j\") # index ranges over 4 values, labelled \"j\"\n",
    "v = randomITensor(j) # random ITensor in R^4 with index j\n",
    "\n",
    "# matrix M_ij\n",
    "i = Index(3, \"i\") # index ranges over 3 values, labelled \"i\"\n",
    "M = randomITensor(i, j) # random ITensor in R^{3x4} with indices i,j\n",
    "\n",
    "# 3-tensor T_ijk\n",
    "k = Index(2, \"k\") # index ranges over 2 values, labelled \"k\"\n",
    "T = randomITensor(i, j, k) # random ITensor in R^{3x4x2} with indices i,j,k\n",
    "\n",
    "# implementing the diagram contractions\n",
    "\n",
    "# mat-vec product u_i = M_ij v_j\n",
    "u = M * v # ITensors automatically contract over shared indices\n",
    "\n",
    "# mat-mat product C_ik = A_ij B_jk\n",
    "A = randomITensor(i, j)\n",
    "B = randomITensor(j, k)\n",
    "C = A * B # contracts over index j\n",
    "\n",
    "# trace of AB, Tr(AB) = A_ij B_ji\n",
    "TrAB = A * B # contracts over index j\n",
    "TrAB = sum(TrAB) # sum over remaining indices to get scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42781b07",
   "metadata": {},
   "source": [
    "<span style=\"color: blue;\">Task:</span> Implement the following tensor contraction.\n",
    "$$ \\mathrm{Tr}(ABCD) = A_{ij} B_{jk} C_{kl} D_{li}, \\quad A, B, C, D \\in \\mathbb{R}^{3 \\times 3},\\quad A_{ij}, B_{jk}, C_{kl}, D_{li} \\sim \\mathcal{N}(0,1) $$\n",
    "Note that we are using Einstein summation convention here. All indices repeat, implying everything is summed over and the result is a scalar (as a trace should be).\n",
    "\n",
    "<details>\n",
    "  <summary> <strong> code </strong> </summary>\n",
    "\n",
    "```julia\n",
    "# redefine i, j, k, l\n",
    "i = Index(3, \"i\")\n",
    "j = Index(3, \"j\")\n",
    "k = Index(3, \"k\")\n",
    "l = Index(3, \"l\")\n",
    "\n",
    "# define random tensors A, B, C, D\n",
    "A = randomITensor(i, j)\n",
    "B = randomITensor(j, k)\n",
    "C = randomITensor(k, l)\n",
    "D = randomITensor(l, i)\n",
    "\n",
    "# compute the trace of ABCD\n",
    "TrABCD = A * B * C * D\n",
    "\n",
    "# sum over all indices to get a scalar\n",
    "TrABCD = sum(TrABCD)\n",
    "\n",
    "println(\"Trace of ABCD:\")\n",
    "println(TrABCD)\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary>tensor diagram</summary>\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"figs/trabcd.jpeg\" width=\"500\"/>\n",
    "</div>\n",
    "  <!-- <img src=\"figs/trabcd.jpeg\" width=\"500\", style=\"centered\"/> -->\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f14232ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inds(u) = ((dim=3|id=455|\"i\"),)\n",
      "inds(C) = ((dim=3|id=455|\"i\"), (dim=2|id=79|\"k\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((dim=3|id=455|\"i\"), (dim=2|id=79|\"k\"))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: try implementing Tr(ABCD) here!\n",
    "\n",
    "# good contraction checks, @show inds\n",
    "@show inds(u) # should show index i\n",
    "@show inds(C) # should show indices i,k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f75d1c",
   "metadata": {},
   "source": [
    "Now, let's use tensor computations for quantum systems.\n",
    "\n",
    "*Curse of Dimensionality.* A chain of $N$ spin-$1/2$ particles has a state space dimension of $\\mathcal H \\cong \\mathbb C^{2^N}$. A many-body state in this space is represented using $2^N$ complex numbers. $$ \\psi_{s_1, s_2, \\ldots, s_N} = \\sum_{s_1, \\ldots, s_N \\in \\{0,1\\}} c_{s_1, s_2, \\ldots, s_N} |s_1\\rangle \\otimes |s_2\\rangle \\otimes \\cdots \\otimes |s_N\\rangle $$\n",
    "\n",
    "\n",
    "For $N = 10$, storing a general state vector requires $2^{10} = 1024$ complex numbers, which is manageable.\n",
    "\n",
    "For $N=50$, storing a general state vector requires $2^{50} \\approx 1.13 \\times 10^{15}$ complex numbers, which is about 9 petabytes of memory!\n",
    "\n",
    "This seems pretty hopeless beyond a few dozen sites. Tensor-networks methods tame this explosion by factorizing $\\psi$ into smaller tensors and keeping *relevant* correlations.\n",
    "\n",
    "$$ \\psi_{s_1, s_2, \\ldots, s_N} = \\sum_{s_1, \\ldots, s_N \\in \\{0,1\\}} c_{s_1, s_2, \\ldots, s_N} |s_1\\rangle \\otimes |s_2\\rangle \\otimes \\cdots \\otimes |s_N\\rangle \\quad \\rightarrow \\quad \\psi_{s_1, s_2, \\ldots, s_N} = A^{[1]}_{s_1} A^{[2]}_{s_2} \\cdots A^{[N]}_{s_N} $$\n",
    "\n",
    "This reduced the storage scaling from $O(d^N)$ to $O(N d m^2)$, where $m$ is the bond dimension controlling the amount of entanglement/correlation captured."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac27820b",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "  <img src=\"figs/mps.png\" alt=\"MPS\" width=\"50%\">\n",
    "  <img src=\"figs/mpsalgo.png\" alt=\"MPS Algorithm\" width=\"50%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d804fbf",
   "metadata": {},
   "source": [
    "*This algorithm is also called the tensor-train singular value decomposition (TT-SVD). Look [here](https://tensornetwork.org/mps/) for a visual introduction and Miles Stoudenmire's [notes](https://https://itensor.org/miles/)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c4d30d1-ad3e-4e41-8b85-69a599643d48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
